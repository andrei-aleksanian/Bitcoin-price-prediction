{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "PATH = os.environ.get(\"PATH_RAW_DATA\")\n",
    "TICKER = os.environ.get(\"TICKER\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Already downloaded using src/utils/fetchStockData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW_DATA = f\"data/data-BTC.json\"\n",
    "PATH_RAW_DATA_CSV = \"data/data-BTC.csv\"\n",
    "print(f\"Looking for {TICKER}\")\n",
    "\n",
    "if not os.path.exists(PATH_RAW_DATA_CSV):\n",
    "  f = open(PATH_RAW_DATA)\n",
    "  data = json.load(f)\n",
    "  data = data['Data'][\"Data\"]\n",
    "  df = pd.DataFrame(columns=['Date','Low','High','Close','Open'])\n",
    "  for v in data:\n",
    "    date = dt.datetime.utcfromtimestamp(v[\"time\"]).strftime('%Y-%m-%d')\n",
    "    data_row = [date,float(v['low']),float(v['high']),\n",
    "                float(v['close']),float(v['open'])]\n",
    "    df.loc[-1,:] = data_row\n",
    "    df.index = df.index + 1\n",
    "  print('Data saved to : %s'%PATH_RAW_DATA_CSV)        \n",
    "  df.to_csv(PATH_RAW_DATA_CSV)\n",
    "\n",
    "# If the data is already there, just load it from the CSV\n",
    "else:\n",
    "    print('File already exists. Loading data from CSV')\n",
    "    df = pd.read_csv(PATH_RAW_DATA_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Preparing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = int(len(df.iloc[:])*0.8)\n",
    "train_data = df.iloc[:train_test_split]\n",
    "test_data = df.iloc[train_test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_data = np.array(train_data.loc[:, \"Close\"]).reshape(-1,1)\n",
    "test_data = np.array(test_data.loc[:, \"Close\"]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Scaler with training data and smooth data\n",
    "smoothing_window_size = 365\n",
    "for di in range(0,int(train_test_split/smoothing_window_size),smoothing_window_size):\n",
    "    scaler.fit(train_data[di:di+smoothing_window_size,:])\n",
    "    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n",
    "\n",
    "# You normalize the last bit of remaining data\n",
    "scaler.fit(train_data[di+smoothing_window_size:,:])\n",
    "train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape both train and test data\n",
    "train_data = train_data.reshape(-1)\n",
    "\n",
    "# Normalize test data\n",
    "test_data = scaler.transform(test_data).reshape(-1)\n",
    "\n",
    "all_close_data = np.concatenate([train_data,test_data],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step)]  \n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif out_end_ix > len(sequence):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)\n",
    " \n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 90, 7\n",
    "# split into samples\n",
    "X, y = split_sequence(train_data, n_steps_in, n_steps_out)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='tanh', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(100, activation='tanh'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "results = model.fit(X, y, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate prediction\n",
    "x_input = test_data[-97:-7]\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = results.history['loss']\n",
    "epochs = range(len(loss))\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(test_data.shape[0]),test_data,color='b',label='True')\n",
    "plt.plot(range(test_data.shape[0]),np.concatenate([test_data[:-7],yhat.reshape(-1)]),color='orange', label='Prediction')\n",
    "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "922f3f0f14de93c55527dbc5ad53b281c54eefd8b075b625f5e98225241459c0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('project-56ryZJ7i')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
