{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "PATH = os.environ.get(\"PATH_RAW_DATA\")\n",
    "TICKER = os.environ.get(\"TICKER\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Already downloaded using src/utils/fetchStockData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW_DATA = f\"data/data-BTC.json\"\n",
    "PATH_RAW_DATA_CSV = \"data/data-BTC.csv\"\n",
    "print(f\"Looking for {TICKER}\")\n",
    "\n",
    "if not os.path.exists(PATH_RAW_DATA_CSV):\n",
    "  f = open(PATH_RAW_DATA)\n",
    "  data = json.load(f)\n",
    "  data = data['Data'][\"Data\"]\n",
    "  df = pd.DataFrame(columns=['Date','Low','High','Close','Open'])\n",
    "  for v in data:\n",
    "    date = dt.datetime.utcfromtimestamp(v[\"time\"]).strftime('%Y-%m-%d')\n",
    "    data_row = [date,float(v['low']),float(v['high']),\n",
    "                float(v['close']),float(v['open'])]\n",
    "    df.loc[-1,:] = data_row\n",
    "    df.index = df.index + 1\n",
    "  print('Data saved to : %s'%PATH_RAW_DATA_CSV)        \n",
    "  df.to_csv(PATH_RAW_DATA_CSV)\n",
    "\n",
    "# If the data is already there, just load it from the CSV\n",
    "else:\n",
    "    print('File already exists. Loading data from CSV')\n",
    "    df = pd.read_csv(PATH_RAW_DATA_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Closing price of Bitcoin over the past 5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(df):\n",
    "  plt.figure(figsize = (18,9))\n",
    "  plt.plot(range(df.shape[0]), df['Close'])\n",
    "  plt.xticks(range(0,df.shape[0],200),df['Date'].loc[::200],rotation=45)\n",
    "  plt.xlabel('Date',fontsize=18)\n",
    "  plt.ylabel('Close Price',fontsize=18)\n",
    "  plt.show()\n",
    "\n",
    "show_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Preparing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = int(len(df.iloc[:])*0.8)\n",
    "train_data = df.iloc[:train_test_split]\n",
    "test_data = df.iloc[train_test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_data = np.array(train_data.loc[:, \"Close\"]).reshape(-1,1)\n",
    "test_data = np.array(test_data.loc[:, \"Close\"]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Scaler with training data and smooth data\n",
    "smoothing_window_size = 365\n",
    "for di in range(0,int(train_test_split/smoothing_window_size),smoothing_window_size):\n",
    "    scaler.fit(train_data[di:di+smoothing_window_size,:])\n",
    "    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n",
    "\n",
    "# You normalize the last bit of remaining data\n",
    "scaler.fit(train_data[di+smoothing_window_size:,:])\n",
    "train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape both train and test data\n",
    "train_data = train_data.reshape(-1)\n",
    "\n",
    "# Normalize test data\n",
    "test_data = scaler.transform(test_data).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now perform exponential moving average smoothing\n",
    "# So the data will have a smoother curve than the original ragged data\n",
    "# EMA = 0.0\n",
    "# gamma = 0.1\n",
    "# for ti in range(train_test_split):\n",
    "#   EMA = gamma*train_data[ti] + (1-gamma)*EMA\n",
    "#   train_data[ti] = EMA\n",
    "\n",
    "# Used for visualization and test purposes\n",
    "all_mid_data = np.concatenate([train_data,test_data],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Averages: SMA and EMA\n",
    "\n",
    "Setting the baseline for the future model with moving averages techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "N = train_data.size\n",
    "std_avg_predictions = []\n",
    "std_avg_x = []\n",
    "mse_errors = []\n",
    "\n",
    "for pred_idx in range(window_size,N):\n",
    "    date = df.loc[pred_idx,'Date']\n",
    "    std_avg_predictions.append(np.mean(train_data[pred_idx-window_size:pred_idx]))\n",
    "    mse_errors.append((std_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "    std_avg_x.append(date)\n",
    "\n",
    "print('MSE error for standard averaging: %.5f'%(0.5*np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\n",
    "plt.plot(range(window_size,N),std_avg_predictions,color='orange',label='Prediction')\n",
    "# plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "N = train_data.size\n",
    "\n",
    "run_avg_predictions = []\n",
    "run_avg_x = []\n",
    "\n",
    "mse_errors = []\n",
    "\n",
    "running_mean = 0.0\n",
    "run_avg_predictions.append(running_mean)\n",
    "\n",
    "decay = 0.5\n",
    "\n",
    "for pred_idx in range(1,N):\n",
    "    running_mean = running_mean*decay + (1.0-decay)*train_data[pred_idx-1]\n",
    "    run_avg_predictions.append(running_mean)\n",
    "    mse_errors.append((run_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "    run_avg_x.append(date)\n",
    "\n",
    "print('MSE error for EMA averaging: %.5f'%(0.5*np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\n",
    "plt.plot(range(0,N),run_avg_predictions,color='orange', label='Prediction')\n",
    "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step)]  \n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.shape)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_step = 100\n",
    "# X_train, y_train = create_dataset(train_data, time_step)\n",
    "# X_test, y_test = create_dataset(test_data, time_step)\n",
    "# # reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "# X_train = X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "# X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "# print(\"X_train: \", X_train.shape)\n",
    "# print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# # Adding a LSTM layer with 10 internal units\n",
    "# model.add(LSTM(100,input_shape=(None,1),activation='relu'))\n",
    "# # Adding a Dense layer with 1 units.\n",
    "# model.add(Dense(1))\n",
    "# # Loss function + optimizer\n",
    "# model.compile(loss='mean_squared_error',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# epochs = range(len(loss))\n",
    "# plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.legend(loc=0)\n",
    "# plt.figure()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_predict=model.predict(X_train)\n",
    "# test_predict=model.predict(X_test)\n",
    "# look_back=time_step\n",
    "\n",
    "# all_predicted_data = np.concatenate([train_predict,test_predict],axis=0)\n",
    "\n",
    "# all_predicted_data = np.concatenate([all_mid_data[:time_step], all_predicted_data.reshape(-1)])\n",
    "\n",
    "# plt.figure(figsize = (18,9))\n",
    "# plt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\n",
    "# plt.plot(range(all_predicted_data.shape[0]),all_predicted_data,color='orange', label='Prediction')\n",
    "# #plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Close Price')\n",
    "# plt.legend(fontsize=18)\n",
    "# plt.show()\n",
    "\n",
    "# # trainPredictPlot = np.empty_like(df[\"Close\"])\n",
    "# # trainPredictPlot[:, :] = np.nan\n",
    "# # trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif out_end_ix > len(sequence):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)\n",
    " \n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 90, 7\n",
    "# split into samples\n",
    "X, y = split_sequence(train_data, n_steps_in, n_steps_out)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='tanh', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(100, activation='tanh'))\n",
    "model.add(Dense(n_steps_out))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "results = model.fit(X, y, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate prediction\n",
    "x_input = test_data[-97:-7]\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = results.history['loss']\n",
    "epochs = range(len(loss))\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(test_data.shape[0]),test_data,color='b',label='True')\n",
    "plt.plot(range(test_data.shape[0]),np.concatenate([test_data[:-7],yhat.reshape(-1)]),color='orange', label='Prediction')\n",
    "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary so far\n",
    "LSTM built!\n",
    "Todo:\n",
    "1. Need to experiment with hypers. E.g. number of days before prediction ??????\n",
    "2. Batch sizes ?\n",
    "3. Need more professional guides on LSTM\n",
    " - try the shampoo tutorial again\n",
    " - look at keras docs\n",
    "4. Develop a robust implementation method\n",
    "\n",
    "\n",
    "ALl these questions can be answered by following keras tutorials - https://keras.io/examples/timeseries/timeseries_weather_forecasting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "922f3f0f14de93c55527dbc5ad53b281c54eefd8b075b625f5e98225241459c0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
